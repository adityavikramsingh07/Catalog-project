# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n95qI9W5L6qaClxxeWEDHB7EjURVMRB8
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("thedevastator/hate-speech-and-offensive-language-detection")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, SpatialDropout1D
from tensorflow.keras.layers import LSTM, Dense, Dropout

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

X = df['tweet']
y = df['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import os
import pandas as pd

file_path = os.path.join(path, "train.csv")
df = pd.read_csv(file_path)
df.head()

df['class'].value_counts().plot(kind='bar')
plt.title("Class Distribution")
plt.show()

import nltk
from nltk.stem import PorterStemmer
import string

stemmer = PorterStemmer()

def clean(text):
    text = str(text).lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    text = text.split()
    text = [stemmer.stem(word) for word in text]
    return " ".join(text)

df["tweet"] = df["tweet"].apply(clean)

tokenizer = Tokenizer(num_words=5000, oov_token="<UNK>")
tokenizer.fit_on_texts(df["tweet"])

sequences = tokenizer.texts_to_sequences(df["tweet"])

X = pad_sequences(
    sequences,
    maxlen=100,
    padding="post",
    truncating="post"
)

y = pd.get_dummies(y).values

import tensorflow as tf

X_train_processed, X_test_processed, y_train_processed, y_test_processed = train_test_split(X, y, test_size=0.2, random_state=42)

# The 'y' variable is already one-hot encoded from the previous step (LxYvqVgW2DL_)
# so we don't need to apply to_categorical here again. If y was not one-hot encoded,
# we would use something like:
# num_classes = len(df['class'].unique())
# y_train_processed = tf.keras.utils.to_categorical(y_train_processed, num_classes=num_classes)
# y_test_processed = tf.keras.utils.to_categorical(y_test_processed, num_classes=num_classes)

print(f"Shape of X_train_processed: {X_train_processed.shape}")
print(f"Shape of X_test_processed: {X_test_processed.shape}")
print(f"Shape of y_train_processed: {y_train_processed.shape}")
print(f"Shape of y_test_processed: {y_test_processed.shape}")

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

model_cnn = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=100),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

model_cnn.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_cnn = model_cnn.fit(
    X_train, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(X_test, y_test)
)

model_lstm = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=100),
    SpatialDropout1D(0.2),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

model_lstm.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_lstm = model_lstm.fit(
    X_train_processed, y_train_processed,
    epochs=5,
    batch_size=32,
    validation_data=(X_test_processed, y_test_processed)
)

model_cnn_lstm = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=100),

    Conv1D(filters=128, kernel_size=5, activation='relu'),
    SpatialDropout1D(0.2),

    LSTM(128, dropout=0.2, recurrent_dropout=0.2),

    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

model_cnn_lstm.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_cnn_lstm = model_cnn_lstm.fit(
    X_train, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(X_test, y_test)
)

bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def bert_encode(texts):
    return bert_tokenizer(
        list(texts),
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

train_encodings = bert_encode(X_train)
test_encodings = bert_encode(X_test)

class HateDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels.tolist()

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = HateDataset(train_encodings, y_train)
test_dataset = HateDataset(test_encodings, y_test)

bert_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",        # Fixed: 'evaluation_strategy' is now 'eval_strategy'
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    logging_dir="./logs",
    save_strategy="no"
)

trainer = Trainer(
    model=bert_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

preds = trainer.predict(test_dataset)
bert_preds = np.argmax(preds.predictions, axis=1)

print("BERT Accuracy:", accuracy_score(y_test, bert_preds))
print(classification_report(y_test, bert_preds))

